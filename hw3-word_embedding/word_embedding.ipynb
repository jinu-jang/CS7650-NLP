{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word_embedding.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUb7_BGivCR9",
        "colab_type": "text"
      },
      "source": [
        "# NLP Homework 3 Programming Assignment\n",
        "\n",
        "## Word Embeddings\n",
        "Word embeddings or word vectors give us a way to use an efficient, dense representation in which similar words have a similar encoding. We have previously seen one-hot vectors used for representing words in a vocabulary. But, unlike these, word embeddings are capable of capturing the context of a word in a document, semantic and syntactic similarity and relation with other words.\n",
        "\n",
        "There are several popular word embeddings that are used, some of them are-\n",
        "- [Word2Vec (by Google)](https://code.google.com/archive/p/word2vec/)\n",
        "- [GloVe (by Stanford)](https://nlp.stanford.edu/projects/glove/)\n",
        "- [fastText (by Facebook)](https://fasttext.cc/)\n",
        "\n",
        "In this assignment, we will be exploring the **word2vec embeddings**, the embedding technique that was popularized by Mikolov et al. in 2013 (refer to the [original paper here](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)). For this, we will be using the GenSim package, find documentation [here](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py). This model is provided by Google and is trained on Google News dataset. Word embeddings from this model have 300 dimensions and are trained on 3 million words and phrases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spG7NfKn0Bpf",
        "colab_type": "text"
      },
      "source": [
        "### Loading word vectors from GenSim\n",
        "Fetch and load the `word2vec-google-news-300` pre-trained embeddings. Note that this may take a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9OYmlcwmHTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import gensim.downloader as api\n",
        "\n",
        "def download_word2vec_embeddings():\n",
        "    print(\"Downloading pre-trained word embeddings from: word2vec-google-news-300.\\n\" \n",
        "          + \"Note: This can take a few minutes.\\n\")\n",
        "    wv = api.load(\"word2vec-google-news-300\")\n",
        "    print(\"\\nLoading complete!\\n\" +\n",
        "          \"Vocabulary size: {}\".format(len(wv.vocab)))\n",
        "    return wv\n",
        "\n",
        "word_vectors = download_word2vec_embeddings()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJzISbkA3xP6",
        "colab_type": "text"
      },
      "source": [
        "The loaded `word_vectors` in memory can be accessed like a dictionary to obtain the embedding of any word, like so-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxgfGGLq5iUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(word_vectors['hello'])\n",
        "print(\"\\nThe embedding has a shape of: {}\".format(word_vectors['hello'].shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hH9Cb2EL7t-e",
        "colab_type": "text"
      },
      "source": [
        "### Finding similar words [5 pts]\n",
        "\n",
        "GenSim provides a simple way out of the box to find the most similar words to a given word. Test this out below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0-g_QH27_Js",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Finding top 5 similar words to 'hello'\")\n",
        "print(word_vectors.most_similar([\"hello\"], topn=5))\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"Finding similarity between 'hello' and 'goodbye'\")\n",
        "print(word_vectors.similarity(\"hello\", \"goodbye\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsC_U2N89PEx",
        "colab_type": "text"
      },
      "source": [
        "For quantifying simiarity between words based on their respective word vectors, a common metric is [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity).  Formally the cosine similarity $s$ between two vectors $a$ and $b$, is defined as:\n",
        "\n",
        "$$s=\\frac{a⋅b}{||a||||b||}, \\text{where }s∈[−1,1]$$\n",
        "\n",
        "**Write your own implementation (using only numpy) of cosine similarity and confirm that it produces the same result as the similarity method available out of the box from GenSim. [3 pts]** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBMfA7rs-o9P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cosine_similarity(vector1, vector2):\n",
        "    ### YOUR CODE BELOW\n",
        "    ### YOUR CODE ABOVE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSHQdWYzBUxB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gensim_similarity = word_vectors.similarity(\"hello\", \"goodbye\")\n",
        "custom_similarity = cosine_similarity(word_vectors['hello'], word_vectors['goodbye'])\n",
        "print(\"GenSim implementation: {}\".format(gensim_similarity))\n",
        "print(\"Your implementation: {}\".format(custom_similarity))\n",
        "\n",
        "assert np.isclose(gensim_similarity, custom_similarity), 'Computed similarity is off from the desired value.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX45u4WOASoc",
        "colab_type": "text"
      },
      "source": [
        "**Additionally, implement two other similarity metrics (using only numpy): [L1 similarity](https://en.wikipedia.org/wiki/Taxicab_geometry) (Manhattan distance) and [L2 similarity](https://en.wikipedia.org/wiki/Euclidean_distance) (Euclidean distance). [2 pts]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Te1xJbnqASMQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def L1_similarity(vector1, vector2):\n",
        "    ### YOUR CODE BELOW\n",
        "    ### YOUR CODE ABOVE\n",
        "\n",
        "def L2_similarity(vector1, vector2):\n",
        "    ### YOUR CODE BELOW\n",
        "    ### YOUR CODE ABOVE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SORtyPzQB7MD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cosine_score = cosine_similarity(word_vectors['hello'], word_vectors['goodbye'])\n",
        "L1_score = L1_similarity(word_vectors['hello'], word_vectors['goodbye'])\n",
        "L2_score = L2_similarity(word_vectors['hello'], word_vectors['goodbye'])                    \n",
        "print(\"Cosine similarity: {}\".format(cosine_score))\n",
        "print(\"L1 similarity: {}\".format(L1_score))\n",
        "print(\"L2 similarity: {}\".format(L2_score))\n",
        "\n",
        "assert np.isclose(cosine_score, 0.63990), 'Cosine similarity is off from the desired value.'\n",
        "assert np.isclose(L1_score, 40.15768), 'L1 similarity is off from the desired value.'\n",
        "assert np.isclose(L2_score, 2.88523), 'L2 similarity is off from the desired value.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MULFRmJfEhka",
        "colab_type": "text"
      },
      "source": [
        "### Exploring synonymns and antonyms [10 pts]\n",
        "\n",
        "In general, you would expect to have a high similarity between synonyms and a low similarity score between antonyms. For e.g. \"pleasant\" would have a higher similarity score to \"enjoyable\" as compared to \"unpleasant\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5xQ30AEFWYd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Similarity between synonyms- 'pleasant' and 'enjoyable': {}\".format(word_vectors.similarity(\"pleasant\", \"enjoyable\")))\n",
        "print(\"Similarity between antonyms- 'pleasant' and 'unpleasant': {}\".format(word_vectors.similarity(\"pleasant\", \"unpleasant\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnu8s16IGDUT",
        "colab_type": "text"
      },
      "source": [
        "However, counter-intuitievely this is not always the case. Often, the similarity score between a word and its antonym is higher than the similarity score with its synonym. For e.g. \"sharp\" has a giher similarity score with \"blunt\" as compared to \"pointed\".\n",
        "\n",
        "**Find two sets of words {$w$, $w_s$, $w_a$} such that {$w$, $w_s$} are synonyms and {$w$, $w_a$} are antonyms, which have intuitive similarity scores with synonyms and antonyms (synonym_score > antonym_score). [4 pts]**\n",
        "\n",
        "**Find two sets of words {$w$, $w_s$, $w_a$} such that {$w$, $w_s$} are synonyms and {$w$, $w_a$} are antonyms, which have counter-intuitive similarity scores with synonyms and antonyms (antonym_score > synonym_score). [4 pts]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJcTOsHBIOHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Similarity between synonyms- 'sharp' and 'pointed': {}\".format(word_vectors.similarity(\"sharp\", \"pointed\")))\n",
        "print(\"Similarity between antonyms- 'sharp' and 'blunt': {}\".format(word_vectors.similarity(\"sharp\", \"blunt\")))\n",
        "\n",
        "### YOUR EXAMPLES BELOW\n",
        "### YOUR EXAMPLES ABOVE\n",
        "\n",
        "print(\"For word set 1:\")\n",
        "syn_score, ant_score = word_vectors.similarity(word_set_1[0], word_set_1[1]), word_vectors.similarity(word_set_1[0], word_set_1[2])\n",
        "print(\"Synonym similarity {} - {}: {}\".format(word_set_1[0], word_set_1[1], syn_score))\n",
        "print(\"Antonym similarity {} - {}: {}\".format(word_set_1[0], word_set_1[2], ant_score))\n",
        "assert syn_score > ant_score, 'word_set_1 is not a valid word set'\n",
        "\n",
        "print(\"For word set 2:\")\n",
        "syn_score, ant_score = word_vectors.similarity(word_set_2[0], word_set_2[1]), word_vectors.similarity(word_set_2[0], word_set_2[2])\n",
        "print(\"Synonym similarity {} - {}: {}\".format(word_set_2[0], word_set_2[1], syn_score))\n",
        "print(\"Antonym similarity {} - {}: {}\".format(word_set_2[0], word_set_2[2], ant_score))\n",
        "assert syn_score > ant_score, 'word_set_2 is not a valid word set'\n",
        "\n",
        "print(\"For word set 3:\")\n",
        "syn_score, ant_score = word_vectors.similarity(word_set_3[0], word_set_3[1]), word_vectors.similarity(word_set_3[0], word_set_3[2])\n",
        "print(\"Synonym similarity {} - {}: {}\".format(word_set_3[0], word_set_3[1], syn_score))\n",
        "print(\"Antonym similarity {} - {}: {}\".format(word_set_3[0], word_set_3[2], ant_score))\n",
        "assert ant_score > syn_score, 'word_set_1 is not a valid word set'\n",
        "\n",
        "print(\"For word set 4:\")\n",
        "syn_score, ant_score = word_vectors.similarity(word_set_4[0], word_set_4[1]), word_vectors.similarity(word_set_4[0], word_set_4[2])\n",
        "print(\"Synonym similarity {} - {}: {}\".format(word_set_4[0], word_set_4[1], syn_score))\n",
        "print(\"Antonym similarity {} - {}: {}\".format(word_set_4[0], word_set_4[2], ant_score))\n",
        "assert ant_score > syn_score, 'word_set_2 is not a valid word set'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkXNkDm6cju3",
        "colab_type": "text"
      },
      "source": [
        "**What do you think is the reason behind this? Look at how the word2vec model is trained and explain your reasoning. [2 pts]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOO44SxWYULx",
        "colab_type": "text"
      },
      "source": [
        "Space for answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjaB9BJaMiS0",
        "colab_type": "text"
      },
      "source": [
        "### Exploring analogies [10 pts]\n",
        "\n",
        "The Distributional Hypothesis which says that words that occur in the same contexts tend to have similar meanings, leads to an interesting property which allows us to find word analogies like \"king\" - \"man\" + \"woman\" = \"queen\".\n",
        "\n",
        "We can exploit this in GenSim like so-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIsQ2jaqNFNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_vectors.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuMhwo_rNPyr",
        "colab_type": "text"
      },
      "source": [
        "In the above, the analogy `man:king::woman:queen` holds true even when looking at the word embeddings.\n",
        "\n",
        "**Find two more such analogies that hold true when looking at embeddings. Write your analogy in the form of `a:b::c:d`, and check that `word_vectors.most_similar(positive=[c, b], negative=[a], topn=1)` produces d. [4 pts]**\n",
        "\n",
        "**Find two cases where the analogies do not hold true when looking at embeddings. Write your analogy in the form of `a:b::c:d`, and check that `word_vectors.most_similar(positive=[c, b], negative=[a], topn=10)` does not have d. [4 pts]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pO81Ar3RGqN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### YOUR EXAMPLES BELOW\n",
        "### YOUR EXAMPLES ABOVE\n",
        "\n",
        "assert(word_vectors.most_similar(positive=[c1, b1], negative=[a1], topn=1))[0][0] == d1, \"example 1 invalid\"\n",
        "assert(word_vectors.most_similar(positive=[c2, b2], negative=[a2], topn=1))[0][0] == d2, \"example 2 invalid\"\n",
        "\n",
        "### YOUR EXAMPLES BELOW\n",
        "### YOUR EXAMPLES ABOVE\n",
        "\n",
        "matches3 = [x for x,y in word_vectors.most_similar(positive=[c3, b3], negative=[a3], topn=10)]\n",
        "matches4 = [x for x,y in word_vectors.most_similar(positive=[c4, b4], negative=[a4], topn=10)]\n",
        "\n",
        "assert d3 not in matches3, \"example 3 invalid\"\n",
        "assert d4 not in matches4, \"example 4 invalid\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhHaj_8-ceP6",
        "colab_type": "text"
      },
      "source": [
        "**Why do you think some analogies work out while some do not? What might be the reason for this? [2 pts]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3cYQVf3YQSe",
        "colab_type": "text"
      },
      "source": [
        "Space for answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH-A8ifMWG5t",
        "colab_type": "text"
      },
      "source": [
        "### Exploring Bias [5 pts]\n",
        "\n",
        "Often, bias creeps into word embeddings. This may be gender, racial or ethnic bias. Let us look at an example-\n",
        "\n",
        "`man:doctor::woman:?`\n",
        "\n",
        "gives high scores for \"nurse\" and \"gynecologist\", revealing the underlying gender stereotypes within these job roles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlsgmwlbWrrO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_vectors.most_similar(positive=[\"woman\", \"doctor\"], negative=[\"man\"], topn=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSUrz7baXw_g",
        "colab_type": "text"
      },
      "source": [
        "**Provide two more examples that reveal some bias in the word embeddings. Look at the top-5 matches and justify your examples. [4 pts]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WW7hpA5HX1f4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### YOUR EXAMPLES BELOW\n",
        "### YOUR EXAMPLES ABOVE\n",
        "\n",
        "print(\"{}:{}::{}:?\".format(a1,b1,c1))\n",
        "print(word_vectors.most_similar(positive=[c1, b1], negative=[a1], topn=5))\n",
        "\n",
        "print(\"\\n{}:{}::{}:?\".format(a2,b2,c2))\n",
        "print(word_vectors.most_similar(positive=[c2, b2], negative=[a2], topn=5))\n",
        "\n",
        "assert d3 not in matches3, \"example 3 invalid\"\n",
        "assert d4 not in matches4, \"example 4 invalid\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eiBykT8cqxB",
        "colab_type": "text"
      },
      "source": [
        "**Why do you think such bias exists? [1 pt]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzdbQIdcYNMw",
        "colab_type": "text"
      },
      "source": [
        "Space for answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUarSe-4eOG-",
        "colab_type": "text"
      },
      "source": [
        "### Visualizing Embeddings [10 pts]\n",
        "\n",
        "Since the word embeddings have a dimension of 300, it is not possible to visualize them directly. However, we can apply a dimension reduction technique like tSNE to reduce the dimensionality of the embeddings to 2-D and then plot them.\n",
        "\n",
        "Visualizing embeddings in this manner allows us to observe semantic and syntactic similarity of words graphically. Words that are similar to each other appear closer to each other on the tSNE plot.\n",
        "\n",
        "Let us begin by loading a smaller dataset and applying the Word2Vec model on that corpus. GenSim has a list of datasets available along with a simple_preprocess utility. You can choose any dataset here for your purpose.\n",
        "\n",
        "We define a `CustomCorpus` class that compiles and loads a dataset of Obama's transcripts (from [here](https://github.com/nlp-compromise/nlp-corpus/tree/master/src/sotu)) and provides it to the Word2Vec model. We then use this model for our tSNE plot later.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLCDWHEdqack",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models.word2vec import Word2Vec\n",
        "from gensim.test.utils import datapath\n",
        "from gensim import utils\n",
        "\n",
        "class CustomCorpus(object):\n",
        "    \"\"\"An interator that yields sentences (lists of str).\"\"\"\n",
        "\n",
        "    def __iter__(self):\n",
        "        # Loading dataset\n",
        "        import urllib.request\n",
        "        urls = [\"https://raw.githubusercontent.com/nlp-compromise/nlp-corpus/master/src/sotu/Obama_2009.txt\",\n",
        "                \"https://raw.githubusercontent.com/nlp-compromise/nlp-corpus/master/src/sotu/Obama_2010.txt\",\n",
        "                \"https://raw.githubusercontent.com/nlp-compromise/nlp-corpus/master/src/sotu/Obama_2011.txt\",\n",
        "                \"https://raw.githubusercontent.com/nlp-compromise/nlp-corpus/master/src/sotu/Obama_2012.txt\",\n",
        "                \"https://raw.githubusercontent.com/nlp-compromise/nlp-corpus/master/src/sotu/Obama_2013.txt\",\n",
        "                \"https://raw.githubusercontent.com/nlp-compromise/nlp-corpus/master/src/sotu/Obama_2014.txt\",\n",
        "                \"https://raw.githubusercontent.com/nlp-compromise/nlp-corpus/master/src/sotu/Obama_2015.txt\"]\n",
        "\n",
        "        compiled = []\n",
        "        for url in urls:\n",
        "            for line in urllib.request.urlopen(url):\n",
        "                compiled.append(line)\n",
        "\n",
        "        # For each line in dataset, yield the preprocessed line\n",
        "        for line in compiled:\n",
        "            yield utils.simple_preprocess(line)\n",
        "\n",
        "model = Word2Vec(sentences=CustomCorpus(), size=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTqE0byakzd3",
        "colab_type": "text"
      },
      "source": [
        "**In the code below, complete the method to generate the tSNE plot, given the word vectors. You may use `sklearn.manifold.TSNE` for this purpose. The `generate_tSNE` method takes as input the original word embedding matrix with shape=(VOCAB_SIZE, 100) and reduces it into a 2-D word embedding matrix with shape=(VOCAB_SIZE, 2). [5 pts]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6E-8nukfXhr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "def generate_tSNE(vectors):\n",
        "    vocab_size = vectors.shape[0]\n",
        "    print(\"Vocab size: {}\".format(vocab_size))\n",
        "    assert vectors.shape[1] == 100\n",
        "\n",
        "    ### YOUR CODE BELOW\n",
        "    ### YOUR CODE ABOVE\n",
        "\n",
        "    assert tsne_transformed_vectors.shape[1] == 2\n",
        "    assert tsne_transformed_vectors.shape[0] == vocab_size \n",
        "    return tsne_transformed_vectors\n",
        "\n",
        "tsne = generate_tSNE(model.wv[model.wv.vocab])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKTkhX_vp0iD",
        "colab_type": "text"
      },
      "source": [
        "Let us plot the result and add labels for a few words on the plot. You can edit the `must_include` list to mandatorily include a few words you want to base your inferences on.\n",
        "\n",
        "**From the tSNE plot, draw inferences for 5 pairs of words, for why they appear close to each other or far apart. Explain your observations with reasoning. [5 pts]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ihnzM8SoT_s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_with_matplotlib(x_vals, y_vals, words, must_include, random_include):\n",
        "    plt.figure(figsize=(15, 15))\n",
        "    plt.scatter(x_vals, y_vals, color=[1., 0.5, 0.5])\n",
        "\n",
        "    indices = list(range(len(words)))\n",
        "    random.seed(1)\n",
        "    selected_indices = random.sample(indices, random_include)\n",
        "    selected_indices.extend([i for i in indices if words[i] in must_include])\n",
        "    for i in selected_indices:\n",
        "        plt.annotate(words[i], (x_vals[i], y_vals[i]), fontsize=12)\n",
        "\n",
        "must_include = []\n",
        "plot_with_matplotlib(tsne[:, 0], tsne[:, 1], list(model.wv.vocab.keys()), must_include, random_include=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWnw1boIdBrY",
        "colab_type": "text"
      },
      "source": [
        "Space for answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZCEhxxHdDWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}